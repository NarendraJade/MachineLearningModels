{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341b4521",
   "metadata": {},
   "source": [
    "Fruit Inspection Quality:\n",
    "\n",
    "Below is a Convolutinal Neural Netowrk(CNN) to classify fruits into fresh and rotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63900b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Found 6253 images belonging to 2 classes.\n",
      "Found 1604 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 1s/step - accuracy: 0.5257 - loss: 0.6903\n",
      "Epoch 2/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81us/step - accuracy: 0.5000 - loss: 0.7243   \n",
      "Epoch 3/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 1s/step - accuracy: 0.6321 - loss: 0.6360\n",
      "Epoch 4/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132us/step - accuracy: 0.7500 - loss: 0.5901  \n",
      "Epoch 5/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 1s/step - accuracy: 0.7417 - loss: 0.5293\n",
      "Epoch 6/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89us/step - accuracy: 0.7188 - loss: 0.5423   \n",
      "Epoch 7/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 2s/step - accuracy: 0.8291 - loss: 0.4010\n",
      "Epoch 8/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115us/step - accuracy: 0.8750 - loss: 0.4139  \n",
      "Epoch 9/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 1s/step - accuracy: 0.8729 - loss: 0.3334\n",
      "Epoch 10/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94us/step - accuracy: 0.8125 - loss: 0.4638   \n",
      "Epoch 11/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - accuracy: 0.8973 - loss: 0.2800\n",
      "Epoch 12/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82us/step - accuracy: 0.9375 - loss: 0.1666   \n",
      "Epoch 13/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 1s/step - accuracy: 0.9068 - loss: 0.2617\n",
      "Epoch 14/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82us/step - accuracy: 0.9062 - loss: 0.3082   \n",
      "Epoch 15/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1100s\u001b[0m 6s/step - accuracy: 0.9282 - loss: 0.2157\n",
      "Epoch 16/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56us/step - accuracy: 0.9375 - loss: 0.2652   \n",
      "Epoch 17/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 1s/step - accuracy: 0.9307 - loss: 0.1930\n",
      "Epoch 18/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81us/step - accuracy: 0.9062 - loss: 0.2521   \n",
      "Epoch 19/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - accuracy: 0.9234 - loss: 0.1981\n",
      "Epoch 20/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83us/step - accuracy: 0.9062 - loss: 0.3896   \n",
      "Epoch 21/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 1s/step - accuracy: 0.9421 - loss: 0.1773\n",
      "Epoch 22/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70us/step - accuracy: 0.8750 - loss: 0.2544   \n",
      "Epoch 23/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - accuracy: 0.9510 - loss: 0.1493\n",
      "Epoch 24/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133us/step - accuracy: 0.8438 - loss: 0.3365  \n",
      "Epoch 25/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 1s/step - accuracy: 0.9409 - loss: 0.1550\n",
      "Epoch 26/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85us/step - accuracy: 0.9375 - loss: 0.1752   \n",
      "Epoch 27/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 1s/step - accuracy: 0.9523 - loss: 0.1312\n",
      "Epoch 28/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69us/step - accuracy: 0.9688 - loss: 0.0551   \n",
      "Epoch 29/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 1s/step - accuracy: 0.9509 - loss: 0.1325\n",
      "Epoch 30/30\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108us/step - accuracy: 1.0000 - loss: 0.0533  \n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 803ms/step - accuracy: 0.8062 - loss: 0.5903\n",
      "Test accuracy: 80.81%\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\Admin\\\\Desktop\\\\GAIP Proj Dataset\\\\Train\\\\Rotten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 142\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Example usage of predict_image_class function\u001b[39;00m\n\u001b[0;32m    141\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mAdmin\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGAIP Proj Dataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mRotten\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 142\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m predict_image_class(image_path)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted class for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 130\u001b[0m, in \u001b[0;36mpredict_image_class\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_image_class\u001b[39m(image_path):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Using data augmentation to make an image classifier work on the data you want to\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    classify'''\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     img \u001b[38;5;241m=\u001b[39m keras_image\u001b[38;5;241m.\u001b[39mload_img(image_path, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m150\u001b[39m))\n\u001b[0;32m    131\u001b[0m     x \u001b[38;5;241m=\u001b[39m keras_image\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n\u001b[0;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Rescale to [0, 1]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:235\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\Admin\\\\Desktop\\\\GAIP Proj Dataset\\\\Train\\\\Rotten'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # A famous library used to create image classifiers amongst other things\n",
    "# Tensorflow keras provides a user-friendly interface to build Deep Neural Networks - A Neural Network with multiple layers\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Check if TensorFlow is using GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Define paths\n",
    "train_dir = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\GAIP Proj Dataset\\\\Train'\n",
    "test_dir = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\GAIP Proj Dataset\\\\Test'\n",
    "\n",
    "# Image Data Generator for augmentation\n",
    "'''Augmentation is done to account for real-world variation (in size, orientation, zoom, etc.),\n",
    "reducing overfitting, generating wider variations'''\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=40,  # randomly rotate between 0 and 40 degrees\n",
    "    width_shift_range=0.2,  # randomly increases or decreases image width by 20%\n",
    "    height_shift_range=0.2,  # randomly increases or decreases image height by 20%\n",
    "    shear_range=0.2,  # randomly shearing (slanting) images towards the left or right up to 20%\n",
    "    zoom_range=0.2,  # zooming in/out up to 20%\n",
    "    horizontal_flip=True,  # horizontal flip\n",
    "    fill_mode='nearest',  # fills empty pixels with the nearest color\n",
    ")\n",
    "\n",
    "# Only rescaling done for test set\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Generators\n",
    "'''train_datagen applies the transformations defined above to the directory \n",
    "   loaded from train_dir using flow_from_directory function\n",
    "   Images are resized to 150x150 pixels\n",
    "   Batches of 32 preprocessed (augmented) images are passed at once to the neural network\n",
    "   This is done to increase memory efficiency and speed\n",
    "   We have 2 labels - fresh and rotten'''\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "'''test_datagen applies the transformations defined above to the directory \n",
    "   loaded from test_dir using flow_from_directory function\n",
    "   Images are resized to 150x150 pixels\n",
    "   Batches of 32 preprocessed (augmented) images are passed at once to the neural network\n",
    "   This is done to increase memory efficiency and speed\n",
    "   We have 2 labels - fresh and rotten'''\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# CNN model\n",
    "'''\n",
    "    1. We initialize a sequential model meaning the neural network's layers are stacked on top of each other.\n",
    "    2. We add the first convolutional layer - a layer that takes an image input\n",
    "       and processes the images to find underlying patterns in order to differentiate it from other classes of images.\n",
    "       Conv2D(32, (3, 3)): Adds a convolutional layer with 32 filters, each of size 3x3.\n",
    "       activation='relu': Uses the ReLU activation function, which helps the model learn complex patterns \n",
    "       by introducing non-linearity.\n",
    "       A filter/kernel is a 3D matrix (of weights) that slides over the image to scan it to extract features of the image. \n",
    "       input_shape=(150, 150, 3): The input images have a size of 150x150 pixels with 3 color channels (RGB).\n",
    "    3. A feature map (another 3D matrix) is the result of applying filter to an input.\n",
    "    4. A max pooling layer reduces the spatial dimensions of a feature Map converting the 3D feature map to say 2D matrix or \n",
    "       an array for faster computation.\n",
    "    5. We repeat steps 2 to 4 thrice again for higher level feature extraction - to obtain abstract level features thereby\n",
    "       refining our understanding. Each layer builds on top of another. Number of filters used is different in each case.\n",
    "    6. Flattens the 3D output of the previous layers to 1D vectors so it can be fed to the fully connected (FC) layers.\n",
    "    7. First Dense - FC layer - Adds a fully connected layer with 512 neurons and uses the ReLU activation function.\n",
    "    8. Adds a dropout layer that randomly sets 50% of the input neurons to zero during each training step. \n",
    "       This helps prevent overfitting by making the network more robust and ensuring it doesn't rely too heavily\n",
    "       on any one feature.\n",
    "    9. Adds a final dense layer with 1 neuron and uses the sigmoid activation function.\n",
    "       This layer outputs a single value between 0 and 1, which is ideal for binary classification tasks \n",
    "       (like classifying between fresh and rotten fruit).\n",
    "       \n",
    "       \n",
    "    So in summary the convolutional and max pooling layers extract features from images while FC layers classify it. \n",
    "      \n",
    "'''\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),  #Adds a convolutional layer with 32 filters, each of size 3x3\n",
    "    MaxPooling2D(2, 2),  # Reduces the spatial dimensions of the feature map\n",
    "    Conv2D(64, (3, 3), activation='relu'),  # Adds another convolutional layer with 64 filters, each of size 3x3\n",
    "    MaxPooling2D(2, 2),  # Reduces the spatial dimensions of the feature map\n",
    "    Conv2D(128, (3, 3), activation='relu'),  # Adds another convolutional layer with 128 filters, each of size 3x3\n",
    "    MaxPooling2D(2, 2),  # Reduces the spatial dimensions of the feature map\n",
    "    Conv2D(128, (3, 3), activation='relu'),  # Adds another convolutional layer with 128 filters, each of size 3x3\n",
    "    MaxPooling2D(2, 2),  # Reduces the spatial dimensions of the feature map\n",
    "    Flatten(),  # Flattens the 3D output to 1D vector\n",
    "    Dense(512, activation='relu'),  # Adds a fully connected layer with 512 neurons using ReLU activation function\n",
    "    Dropout(0.5),  # Adds dropout to prevent overfitting\n",
    "    Dense(1, activation='sigmoid')  # Adds the final output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "'''1. Loss function is a measure of how wrong the model's predictions are compared to the actual outputs (labels).\n",
    "      'binary_crossentropy' is used when the model is predicting between two choices (like yes/no or 0/1).\n",
    "   2. Optimizer is how the model updates itself based on the data it sees and the loss function.\n",
    "      Adam is just a fancy name for a specific way of doing this, and 'learning_rate=1e-4' \n",
    "      means the model will learn slowly at first, which can sometimes help it learn better.\n",
    "   3. 'accuracy' is a common way to measure how often the model predicts correctly.\n",
    "'''\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "# Epoch refers to the number of cycles implemented during training.\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=30\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Function to predict image class\n",
    "def predict_image_class(image_path):\n",
    "    '''Using data augmentation to make an image classifier work on the data you want to\n",
    "    classify'''\n",
    "    img = keras_image.load_img(image_path, target_size=(150, 150))\n",
    "    x = keras_image.img_to_array(img)\n",
    "    x = x / 255.0  # Rescale to [0, 1]\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "    prediction = model.predict(x)\n",
    "    if prediction < 0.5:\n",
    "        return 'Fresh'\n",
    "    else:\n",
    "        return 'Rotten'\n",
    "\n",
    "# Example usage of predict_image_class function\n",
    "image_path = '\"C:\\Users\\Admin\\Desktop\\GAIP Proj Dataset\\Train\\Rotten\\1 (2).jpg\"'\n",
    "predicted_class = predict_image_class(image_path)\n",
    "print(f\"Predicted class for {image_path}: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8975cef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('/content/drive/MyDrive/GAIP Proj Dataset/fruit_freshness_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
